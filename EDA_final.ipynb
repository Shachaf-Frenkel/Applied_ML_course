{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "diabetes_130_us_hospitals_for_years_1999_2008 = fetch_ucirepo(id=296) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = diabetes_130_us_hospitals_for_years_1999_2008.data.features \n",
    "y = diabetes_130_us_hospitals_for_years_1999_2008.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(diabetes_130_us_hospitals_for_years_1999_2008.metadata) \n",
    "# variable information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(diabetes_130_us_hospitals_for_years_1999_2008.keys())\n",
    "a = diabetes_130_us_hospitals_for_years_1999_2008['data']\n",
    "(X['admission_type_id']== 2).sum()\n",
    "df =a['features']\n",
    "a['ids']\n",
    "#df = pd.concat([a['ids'],a['features']],axis = 1)\n",
    "\n",
    "data = df.copy()\n",
    "display(data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_train_test_considering_patient_nbr(data):\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    diabetes_130_us_hospitals_for_years_1999_2008 = fetch_ucirepo(id=296)\n",
    "    #id_patients = diabetes_130_us_hospitals_for_years_1999_2008.data.ids\n",
    "    #full_dataset = diabetes_130_us_hospitals_for_years_1999_2008.data.original\n",
    "    id_patients = data['ids']\n",
    "    full_dataset = data['original']\n",
    "\n",
    "    # Identify unique patient numbers\n",
    "    unique_patient_nbrs = id_patients['patient_nbr'].unique()\n",
    "    # Split patient numbers into train and test sets\n",
    "    train_patient_nbrs, test_patient_nbrs = train_test_split(unique_patient_nbrs, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Filter rows based on train and test patient numbers\n",
    "    train_set = full_dataset[full_dataset['patient_nbr'].isin(train_patient_nbrs)]\n",
    "    test_set = full_dataset[full_dataset['patient_nbr'].isin(test_patient_nbrs)]\n",
    "\n",
    "    # Check that there's no id in the train set that is also in the test set:\n",
    "    list_train_patients_id = list(train_set[\"patient_nbr\"])\n",
    "    uniques_list_train = set(list_train_patients_id)\n",
    "    list_test_patients_id = list(test_set[\"patient_nbr\"])\n",
    "    uniques_list_test = set(list_test_patients_id)\n",
    "    for i in list(uniques_list_train):\n",
    "        if i in list(uniques_list_test):\n",
    "            print(\"problem - id both in train and test\")\n",
    "\n",
    "    return train_set,test_set\n",
    "\n",
    "dataset = [train_set,test_set] = split_train_test_considering_patient_nbr(a)\n",
    "\n",
    "\n",
    "# check statistics of labels in training and test:\n",
    "train_readmitted_stat = dataset[0][\"readmitted\"].value_counts() / len(dataset[0])\n",
    "print(\"training dataset readmitted statistics\", train_readmitted_stat)\n",
    "test_readmitted_stat =dataset[1][\"readmitted\"].value_counts() / len(dataset[1])\n",
    "print(\"test dataset readmitted statistics\", test_readmitted_stat)\n",
    "# Check statistics of gender in training and test:\n",
    "train_gender_stat = dataset[0][\"gender\"].value_counts() / len(dataset[0])\n",
    "print(\"training dataset gender statistics\", train_gender_stat)\n",
    "test_gender_stat =dataset[1][\"gender\"].value_counts() / len(dataset[1])\n",
    "print(\"test dataset gender statistics\", test_gender_stat)\n",
    "train_set"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filtering data: For the subplots of 'diag_1', 'diag_2', 'diag_3', and 'medical_specialty', ,'num_lab_procedures', 'num_medications' where the number of distinct values is between 848-954,\n",
    "# reading the x-labels and visually differentiating between columns becomes challenging. In these subplots, we will visualize only the top 20 common diagnoses.\n",
    "columns_of_interest = ['diag_1', 'diag_2', 'diag_3', 'medical_specialty','num_lab_procedures', 'num_medications']\n",
    "# Get the top 20 categories for each column\n",
    "top_20_categories = {}\n",
    "for column in columns_of_interest:\n",
    "    top_20_categories[column] = train_set[column].value_counts().nlargest(20).index\n",
    "# Filter the DataFrame to include only the rows with the top 20 categories for each column\n",
    "filtered_data = train_set[\n",
    "    (train_set['diag_1'].isin(top_20_categories['diag_1'])) &\n",
    "    (train_set['diag_2'].isin(top_20_categories['diag_2'])) &\n",
    "    (train_set['diag_3'].isin(top_20_categories['diag_3'])) &\n",
    "    (train_set['medical_specialty'].isin(top_20_categories['medical_specialty'])) &\n",
    "    (train_set['num_lab_procedures'].isin(top_20_categories['num_lab_procedures'])) &\n",
    "    (train_set['num_medications'].isin(top_20_categories['num_medications']))]\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creating a list of the numerical and categorical features\n",
    "numerical = list(train_set.select_dtypes(include=['int64', 'float64']).drop(columns=['discharge_disposition_id','admission_type_id','admission_source_id', 'encounter_id', 'patient_nbr']).columns)\n",
    "categorical = list(train_set.select_dtypes(include=['object']).drop(columns= ['payer_code']).columns)\n",
    "rcol_names_list = filtered_data.columns.tolist()\n",
    "remove_list = ['admission_type_id','discharge_disposition_id','encounter_id','admission_type_id','admission_source_id','payer_code','patient_nbr']\n",
    "col_names_list = [i for i in rcol_names_list if i not in remove_list]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function for plotting our data\n",
    "def plotting(plot_type, C_list, C_data, the_label=None):\n",
    "    lenl = len(C_list)//3\n",
    "    fig, ax = plt.subplots(lenl, 3, figsize=(20,5*lenl))\n",
    "    for variable, subplot in zip(C_list, ax.flatten()):\n",
    "        if the_label == None:\n",
    "            # function for plotting counts of multiple variables\n",
    "            plot_type(x=C_data[variable], ax=subplot, color='#86bf91', zorder=2, width=0.9)\n",
    "            # customize plot desighn\n",
    "            subplot.set_ylabel(\"Count\", labelpad=8, size=12)\n",
    "            subplot.set_xlabel(variable, labelpad=8, size=12)\n",
    "            # get the total count of the type column\n",
    "            total = C_data[variable].count()\n",
    "            subplot.bar_label(subplot.containers[0], fmt=lambda x: f'{(x/total)*100:0.1f}%')\n",
    "            # add space at the end of the bar for the labels\n",
    "            subplot.margins(x=0.01)\n",
    "\n",
    "        else:\n",
    "            if plot_type == sns.heatmap:\n",
    "                heatmap_data = pd.crosstab(index=C_data[variable], columns=C_data[the_label])\n",
    "                plot_type(data=heatmap_data, cmap=\"crest\")\n",
    "\n",
    "            else:\n",
    "                # plotting multiple variables against one variable\n",
    "                plot_type(x=C_data[the_label], y=C_data[variable], ax=subplot, color='#86bf91', zorder=2, width=0.9)\n",
    "\n",
    "            # customize plot desighn\n",
    "            subplot.set_ylabel(variable, labelpad=8, size=12)\n",
    "            subplot.set_xlabel(the_label, labelpad=8, size=12)\n",
    "        subplot.spines['right'].set_visible(False)\n",
    "        subplot.spines['top'].set_visible(False)\n",
    "\n",
    "        if C_list==categorical:\n",
    "            # Rotating x-axis labels for better readability\n",
    "            for label in subplot.get_xticklabels():\n",
    "                label.set_rotation(90)\n",
    "\n",
    "    plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initial analysis\n",
    "### Let's look at the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis (visualizing single variables)\n",
    "### We atart with looking at the attributes' datatypes and number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descriptive statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assessment of Missing Values\n",
    "### Per Column and Overall Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = train_set.isnull().sum()\n",
    "\n",
    "# Display columns with missing values (if any)\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "if not columns_with_missing_values.empty:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(columns_with_missing_values)\n",
    "else:\n",
    "    print(\"No missing values found in any column.\")\n",
    "\n",
    "full_data_nan = train_set.isna().sum().sum()\n",
    "data_shape = train_set.shape[0]*train_set.shape[1]\n",
    "full_nans_perc = full_data_nan/data_shape*100\n",
    "\n",
    "print('Missing values percentage in the dataset:',full_nans_perc,'%')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram Displaying Percentage of Missing Values Across Features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adjusting the 'none' value in two columns to 'not measured' - it is not a missing value by meaning\n",
    "train_set['max_glu_serum'] = train_set['max_glu_serum'].fillna('not measured')\n",
    "train_set['A1Cresult'] = train_set['A1Cresult'].fillna('not measured')\n",
    "train_set['readmitted'] = train_set['readmitted'].fillna('NO')\n",
    "train_set['max_glu_serum']\n",
    "\n",
    "# Calculate the percentage of missing values per column\n",
    "nan_percentages = train_set.isnull().mean() * 100\n",
    "\n",
    "# Filter columns with missing values and their percentages\n",
    "filtered_columns = nan_percentages[nan_percentages > 0].index\n",
    "nan_percentage = nan_percentages[nan_percentages > 0].values\n",
    "\n",
    "nans = sns.barplot(x=filtered_columns , y=nan_percentage, color='#86bf91', zorder=2, width=0.9)\n",
    "# customize plot design\n",
    "nans.set_ylabel(\"Missing values percentage (%)\", labelpad=8, size=12)\n",
    "nans.spines['right'].set_visible(False)\n",
    "nans.spines['top'].set_visible(False)\n",
    "\n",
    "for index, value in enumerate(nan_percentage):\n",
    "    nans.text(index, value + 0.5, f'{value:.1f}%', ha='center', color='black')\n",
    "\n",
    "# Rotating x-axis labels for better readability\n",
    "for label in nans.get_xticklabels():\n",
    "    label.set_rotation(90)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing categorical variables\n",
    "### Categorical variables distribution visualized using histogram and to visualize unique values and outliers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotting(sns.countplot, categorical, filtered_data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing numerical variables\n",
    "### Plotting numerical feature counts using histogram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotting(sns.countplot, numerical, filtered_data)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing relationship between variables\n",
    "### Plotting heatmap for pearson correlation between numerical variables indluding the label (readmitted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set['readmitted'] = train_set['readmitted'].apply(lambda x : 1 if x == '>30'\n",
    "                                                            else ( 0 if x == '<30'\n",
    "                                                            else ( None)))\n",
    "numerical.append('readmitted')\n",
    "cor_df_num = train_set[numerical].corr(method = \"pearson\").round(2)\n",
    "sns.heatmap(cor_df_num, vmin=0, vmax=1, annot=True, cmap=\"crest\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the intersection of readmission status with categorical features to uncover correlations between these features and the label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the readmittion rate for of the relevant drugs dist:\n",
    "train_set.loc[train_set['readmitted']=='>30','readmitted']='other'\n",
    "train_set.loc[train_set['readmitted']=='NO','readmitted']='other'\n",
    "\n",
    "lenl = len(categorical)//3\n",
    "fig, ax = plt.subplots(lenl, 3, figsize=(20,5*lenl))\n",
    "for variable, subplot in zip(categorical, ax.flatten()):\n",
    "    sns.histplot(ax=subplot ,data=train_set, x=train_set[variable], hue=\"readmitted\", stat='density', common_norm=False, binwidth=1,\n",
    "                 kde=False, kde_kws={'bw_adjust': 5}, color='#86bf91')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# another visualization for categorical features and the label: 3 features example\n",
    "exmp_cat = ['age', 'medical_specialty', 'insulin']\n",
    "for cat in exmp_cat:\n",
    "    # Assuming df is your DataFrame\n",
    "    set_age = filtered_data[cat]\n",
    "    set_age.value_counts()\n",
    "\n",
    "    # Group by colA and then apply value_counts on colB within each group\n",
    "    result = filtered_data.groupby(cat)['readmitted'].value_counts(normalize=False)\n",
    "\n",
    "    # Sort a specific result from colB inside the result variable\n",
    "    sorted_result = result[:, '<30'].sort_values(ascending=False)\n",
    "    sorted_result\n",
    "\n",
    "    # Get the value counts of colA\n",
    "    value_counts_colA = set_age.value_counts()\n",
    "\n",
    "    # Calculate the length of each relevant value count of colA and divide the sorted_result by it\n",
    "    sorted_result_normalized = sorted_result / value_counts_colA\n",
    "    sorted_result_normalized\n",
    "\n",
    "    # Assuming sorted_result_normalized is the Series containing the normalized rates\n",
    "\n",
    "    # Plot the data with Seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=sorted_result_normalized.index, y=sorted_result_normalized.values, marker='o',color='#86bf91')\n",
    "\n",
    "    # Add labels and title\n",
    "\n",
    "    plt.ylabel('Normalized readmission rate within 30 days')\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    # Add dotted grid lines\n",
    "    plt.grid(True, linestyle='--')\n",
    "    # Rotating x-axis labels for better readability\n",
    "\n",
    "    for label in plt.gca().get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Quality and cleanliness:\n",
    "### Plotting cumulative Maximum Occurrence and Percentage per Column for Outlier Detection Impact"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to find category with max occurrence and its percentage for each column\n",
    "def find_max_occurrence_and_percentage(column):\n",
    "    value_counts = column.value_counts()\n",
    "    max_category = value_counts.idxmax()\n",
    "    max_count = value_counts.max()\n",
    "    total_count = column.count()\n",
    "    percentage = (max_count / total_count) * 100\n",
    "    return max_category, percentage\n",
    "\n",
    "# Iterate over columns and find category with max occurrence and percentage\n",
    "result = {}\n",
    "for column in categorical:\n",
    "    max_category, percentage = find_max_occurrence_and_percentage(train_set[column])\n",
    "    result[column] = percentage\n",
    "\n",
    "#Comulative Histogram - The Pecentage of Occoarance in the Most Common Catagory\n",
    "h = sns.histplot(result.values(),binwidth=5,binrange = [0,100],cumulative = True,legend=False, color='#86bf91', zorder=2)\n",
    "plt.vlines(x=95, ymin=0, ymax=40, colors='r', linestyles='dashed')\n",
    "plt.xlabel('Pecentage of occurrence in the most common catagory [%]')\n",
    "plt.title('Comulative Histogram - The Pecentage of occurrence in the Most Common Catagory')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "#filter by threshold\n",
    "filtered_keys = [key for key, value in result.items() if value < 95]\n",
    "\n",
    "# 'Histogram of Missing Values Percaentage'\n",
    "plt.figure()\n",
    "n_subjects = len(df.index)\n",
    "df_new = pd.Series(df.isna().sum()).reset_index(drop = True)/n_subjects*100\n",
    "sns.histplot(df_new,binwidth= 5,cumulative = False,legend=False,binrange = [0,100],color='#86bf91', zorder=2)\n",
    "plt.xlabel('Missing values [%]')\n",
    "plt.title('Histogram of Missing Values Percaentage')\n",
    "plt.vlines(x=30, ymin=0, ymax=47, colors='r', linestyles='dashed')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quantifying Data Loss per Outlier Quantile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to remove outliers using the IQR method for a specific column\n",
    "def remove_outliers_iqr(df, column, q=0.10):\n",
    "    # Calculate the 10th and 90th percentiles\n",
    "    lower_bound = df[column].quantile(q)\n",
    "    upper_bound = df[column].quantile(1-q)\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Plot the percentage of data lost for each quantile to set a right outliers drop for our data\n",
    "\n",
    "quantiles = [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]  # Quantiles to calculate\n",
    "percent_lost_dict = {}\n",
    "\n",
    "# Calculate the percentage of numerical data lost for different quantiles\n",
    "for q in quantiles:\n",
    "    temp_data = train_set[numerical].copy()  # Make a copy of the original data\n",
    "    for column in numerical:\n",
    "        outliers = remove_outliers_iqr(temp_data, column, q)\n",
    "        temp_data.drop(outliers.index, inplace=True)\n",
    "    percent_lost_dict[q] = ((len(train_set[numerical]) - len(temp_data)) / len(train_set[numerical])) * 100\n",
    "\n",
    "# Plot the percentage of data lost for each quantile\n",
    "ax = sns.lineplot(x=list(percent_lost_dict.keys()), y=list(percent_lost_dict.values()), marker='o', color='#86bf91', zorder=2)\n",
    "plt.title('Percentage of numerical Data Lost for Different Quantiles')\n",
    "plt.xlabel('Quantile (%)')\n",
    "plt.ylabel('Percentage of Data Lost (%)')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_xlim(left=0)  # Set the lower limit of x-axis to 0\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
